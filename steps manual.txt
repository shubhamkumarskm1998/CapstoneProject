
open on webshell & login to Webshell:
step 1. Login to mysql
mysql -u anabig11415 -pBigdata123

show databases;
use anabig114215;

step 2. Create tables for retail data using codes
a. upload create_db.sql to ftp (https://npbdh.cloudloka.com/ftp
b. run the below command to create tables under 
source /home/anabig114215/create_db.sql


step 3. Understand sqoop commands
sqoop list-databases --connect jdbc:mysql://ip-10-1-1-204.ap-south-1.compute.internal:3306 --username anabig114215 --password Bigdata123

sqoop list-tables --connect jdbc:mysql://ip-10-1-1-204.ap-south-1.compute.internal:3306/anabig114215 --username anabig114215 --password Bigdata123

-- to save in avro file
sqoop import-all-tables  --connect jdbc:mysql://ip-10-1-1-204.ap-south-1.compute.internal:3306/anabig114215 --username anabig114215 --password Bigdata123 --compression-codec=snappy --as-avrodatafile --warehouse-dir=/user/anabig114215/hive/warehouse/finalrun4 --m 1 --driver com.mysql.jdbc.Driver 

to save in parquet file
sqoop import-all-tables  --connect jdbc:mysql://ip-10-1-1-204.ap-south-1.compute.internal:3306/anabig114215 --username anabig114215 --password Bigdata123 --compression-codec=snappy --as-parquetfile --warehouse-dir=/user/anabig114215/hive/warehouse/parquetfile --m 1  --driver com.mysql.jdbc.Driver 



step 4. Check the data is moved to HDFS or not
hdfs dfs -ls /user/anabig114215/hive/warehousefinalrun  /// for just avro i'm suing avro file here


Check the avsc schema files created or not  // these are the schema files we will also .java files as well which i basically java code how this file is import via sqoop 
ls -l /home/anabig114215/*.avsc


step 5. Open shell and run the below commands


hadoop fs -mkdir /user/anabig114215/capst
hadoop fs -put /home/anabig114215/Employees.avsc /user/anabig114215/hive/warehouse/capst/
hadoop fs -put /home/anabig114215/Departments.avsc /user/anabig114215/hive/warehouse/capst/
hadoop fs -put /home/anabig114215/Department_Employees.avsc /user/anabig114215/hive/warehouse/capst/
hadoop fs -put /home/anabig114215/Depaartment_managers.avsc /user/anabig114215/hive/warehouse/capst/
hadoop fs -put /home/anabig114215/Salaries.avsc /user/anabig114215/hive/warehouse/capst/
hadoop fs -put /home/anabig114215/Titles.avsc /user/anabig114215/hive/warehouse/capst/

hive -f command_hive.sql > outputCapstone1.txt

so using the data which is the input then transfering to HDFS via scoop (data ingestion) 
then to data processing in Hadoop MR then data analysis using hive then to store outputs of queries to text file

for Pipeline

after step 2 run the .sh file 
step 3: sh cap1.sh

step 4: 
impala-shell -i ip-10-1-2-103.ap-south-1.compute.internal -d shubham -f command_impala.sql > outputimpala.txt   
hive -f command_hive.sql >output_hive.txt  




