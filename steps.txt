
open on webshell & login to Webshell:
1. Login to mysql
mysql -u anabig11415 -pBigdata123

show databases;
use anabig114215;

2. Create tables for retail data using codes
a. upload create_db.sql to ftp (https://npbdh.cloudloka.com/ftp
b. run the below command to create tables under 
source /home/anabig114215/create_db.sql


3. Understand sqoop commands
sqoop list-databases --connect jdbc:mysql://ip-10-1-1-204.ap-south-1.compute.internal:3306 --username anabig114215 --password Bigdata123

sqoop list-tables --connect jdbc:mysql://ip-10-1-1-204.ap-south-1.compute.internal:3306/anabig114215 --username anabig114215 --password Bigdata123

-- to save in avro file
sqoop import-all-tables  --connect jdbc:mysql://ip-10-1-1-204.ap-south-1.compute.internal:3306/anabig114215 --username anabig114215 --password Bigdata123 --compression-codec=snappy --as-avrodatafile --warehouse-dir=/user/anabig114215/hive/warehouse/finalrun4 --m 1 --driver com.mysql.jdbc.Driver 

to save in parquet file
sqoop import-all-tables  --connect jdbc:mysql://ip-10-1-1-204.ap-south-1.compute.internal:3306/anabig114215 --username anabig114215 --password Bigdata123 --compression-codec=snappy --as-parquetfile --warehouse-dir=/user/anabig114215/hive/warehouse/parquetfile --m 1  --driver com.mysql.jdbc.Driver 



4. Check the data is moved to HDFS or not
hdfs dfs -ls /user/anabig114215/hive/warehousefinalrun  /// for just avro i'm suing avro file here


Check the avsc schema files created or not  // these are the schema files we will also .java files as well which i basically java code how this file is import via sqoop 
ls -l /home/anabig114215/*.avsc


5. Open shell and run the below commands


hadoop fs -mkdir /user/anabig114215/new
hadoop fs -put /home/anabig114215/Employees.avsc /user/anabig114215/hive/warehouse/capstone3/Employees.avsc
hadoop fs -put /home/anabig114215/Departments.avsc /user/anabig114215/hive/warehouse/capstone3/Departments.avsc
hadoop fs -put /home/anabig114215/Department_Employees.avsc /user/anabig114215/hive/warehouse/capstone3/Department_Employees.avsc
hadoop fs -put /home/anabig114215/Depaartment_managers.avsc /user/anabig114215/hive/warehouse/capstone3/Depaartment_managers.avsc
hadoop fs -put /home/anabig114215/Salaries.avsc /user/anabig114215/capstone3/hive/warehouse/Salaries.avsc
hadoop fs -put /home/anabig114215/Titles.avsc /user/anabig114215/capstone3/hive/warehouse/Titles.avsc
hadoop fs -mkdir /user/anabig114215/hive/warehouse/finalrun3         ///here check the data which you have import using sqoop .avro files will be there


hive -f command_hive.sql > outputCapstone1.txt

so using the data which is the input then transfering to HDFS via scoop (data ingestion) 
then to data processing in Hadoop MR then data analysis using hive then to store outputs of queries to text file




